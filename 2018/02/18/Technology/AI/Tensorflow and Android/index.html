<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="Android,AI,Tensorflow," />





  <link rel="alternate" href="/atom.xml" title="For life" type="application/atom+xml" />






<meta name="description" content="In the age of mobile Internet and artificial intelligence, we should follow in Google’s footsteps.">
<meta property="og:type" content="article">
<meta property="og:title" content="Tensorflow and Android">
<meta property="og:url" content="http://deepcreator.com/2018/02/18/Technology/AI/Tensorflow and Android/index.html">
<meta property="og:site_name" content="For life">
<meta property="og:description" content="In the age of mobile Internet and artificial intelligence, we should follow in Google’s footsteps.">
<meta property="og:image" content="http://opb37u589.bkt.clouddn.com/5119a-1kyuv2e6hruh8_b9jm1c7ha.png?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/749674-18941f05fd616658.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/749674-0775a2794ea37717.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700">
<meta property="og:image" content="https://upload-images.jianshu.io/upload_images/749674-81025bb04ad038a2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/700">
<meta property="og:image" content="http://opb37u589.bkt.clouddn.com/ml_android.png?imageMogr2/thumbnail/!75p">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/keyboard_example.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/pen_example.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/wallet_example.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/sample_combined.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/1.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/2.png">
<meta property="og:image" content="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/3.png">
<meta property="og:image" content="https://github.com/zeusees/HyperLPR/blob/master/demo_images/test.png">
<meta property="og:image" content="https://github.com/zeusees/HyperLPR/raw/master/demo_images/15.jpg">
<meta property="og:image" content="https://github.com/zeusees/HyperLPR/raw/master/demo_images/android.png">
<meta property="og:image" content="https://github.com/sundyCoder/hyperlpr4Android/raw/master/hyperlpr.jpg">
<meta property="og:image" content="https://camo.githubusercontent.com/253d86d976cfae14e1b02f02e30179fc0d6b01d7/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6b46574b644c4f78796b452f687164656661756c742e6a7067">
<meta property="og:image" content="https://github.com/mari-linhares/mnist-android-tensorflow/raw/master/images/demo.png">
<meta property="og:image" content="https://camo.githubusercontent.com/fcdf8f032e14ba941e9e387c87ff0c5cb835113b/687474703a2f2f692e696d6775722e636f6d2f68736b64766f692e706e67">
<meta property="og:image" content="https://camo.githubusercontent.com/c7524a5212d3212f0c017a50cac6c765e1389964/687474703a2f2f6e6172722e6a702f707269766174652f6d69796f7368692f74656e736f72666c6f772f74656e736f72666c6f775f73637265656e312e706e67">
<meta property="og:image" content="https://camo.githubusercontent.com/3a15c68b040d69a6a870c6207b838fce313aff0a/687474703a2f2f6e6172722e6a702f707269766174652f6d69796f7368692f74656e736f72666c6f772f6d6e6973745f73637265656e302e706e67">
<meta property="og:image" content="https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/raw/master/images/demo.png">
<meta property="og:image" content="https://github.com/akirasosa/mobile-semantic-segmentation/raw/master/assets/prediction.png">
<meta property="og:image" content="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/start.gif">
<meta property="og:image" content="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/rcn2.gif">
<meta property="og:image" content="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/rcn1.gif">
<meta property="og:image" content="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/view.gif">
<meta property="og:image" content="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/take-photo.gif">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/screenshots/1.png">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/screenshots/2.png">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/screenshots/3.png">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/screenshots/4.png">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/screenshots/5.png">
<meta property="og:image" content="https://github.com/alseambusher/Paideia/raw/master/res/drawable-xxxhdpi/ic_launcher.png">
<meta property="og:image" content="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/hangul_tensordroid_demo1.gif">
<meta property="og:image" content="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/architecture.png">
<meta property="og:image" content="https://camo.githubusercontent.com/cff8a356ace2a9ebd6aeb441d4bb8b68395606f8/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6965665961434f7a3030732f302e6a7067">
<meta property="og:image" content="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/android-project-structure.png">
<meta property="og:image" content="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/android-studio-play-button.png">
<meta property="og:image" content="https://github.com/alvarowolfx/ai-candy-dispenser/raw/master/Schematic.png">
<meta property="og:image" content="https://github.com/gokhanettin/driverless-rccar/raw/master/img/rccar.jpg">
<meta property="og:image" content="https://github.com/gokhanettin/driverless-rccar/raw/master/img/data-acquisition.jpg">
<meta property="og:image" content="https://github.com/gokhanettin/driverless-rccar/raw/master/img/dataset.jpg">
<meta property="og:image" content="https://camo.githubusercontent.com/d7c922af7b5102f1af949409b9b22ff951a8fcba/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f5a36564b646f46325a58592f302e6a7067">
<meta property="og:updated_time" content="2018-03-23T01:34:38.341Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Tensorflow and Android">
<meta name="twitter:description" content="In the age of mobile Internet and artificial intelligence, we should follow in Google’s footsteps.">
<meta name="twitter:image" content="http://opb37u589.bkt.clouddn.com/5119a-1kyuv2e6hruh8_b9jm1c7ha.png?imageMogr2/thumbnail/!75p">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.3',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://deepcreator.com/2018/02/18/Technology/AI/Tensorflow and Android/"/>





  <title>Tensorflow and Android | For life</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?5aaaaf5c9bcea80a5c7d7a057b2d36b3";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta custom-logo">
    
      <div class="site-meta-headline">
        <a>
          <img class="custom-logo-image" src="/img/headonhill.jpg"
               alt="For life"/>
        </a>
      </div>
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">For life</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Keep Growing Up is the only Password.</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-help">
          <a href="/404.html" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br />
            
            help
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://deepcreator.com/2018/02/18/Technology/AI/Tensorflow and Android/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="IPCreator">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/img/headonhill.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="For life">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Tensorflow and Android</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-02-18T17:21:06+08:00">
                2018-02-18
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/TensorFlow/" itemprop="url" rel="index">
                    <span itemprop="name">TensorFlow</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
            <!--noindex-->
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/02/18/Technology/AI/Tensorflow and Android/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count hc-comment-count" data-xid="2018/02/18/Technology/AI/Tensorflow and Android/" itemprop="commentsCount"></span>
                </a>
              </span>
              <!--/noindex-->
            
          

          
          
             <span id="/2018/02/18/Technology/AI/Tensorflow and Android/" class="leancloud_visitors" data-flag-title="Tensorflow and Android">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p><img src="http://opb37u589.bkt.clouddn.com/5119a-1kyuv2e6hruh8_b9jm1c7ha.png?imageMogr2/thumbnail/!75p" alt="5119a-1kyuv2e6hruh8_b9jm1c7ha.png"></p>
<blockquote>
<p>In the age of mobile Internet and artificial intelligence, we should follow in Google’s footsteps.</p>
</blockquote>
<a id="more"></a>
<h1 id="开源网站：Github"><a href="#开源网站：Github" class="headerlink" title="开源网站：Github"></a><a href="https://github.com/search?o=desc&amp;q=tensorflow+android&amp;s=forks&amp;type=Repositories&amp;utf8=%E2%9C%93" target="_blank" rel="external">开源网站：Github</a></h1><h2 id="TSFOnAndroid"><a href="#TSFOnAndroid" class="headerlink" title="TSFOnAndroid"></a><a href="https://github.com/CrystalChen1017/TSFOnAndroid" target="_blank" rel="external">TSFOnAndroid</a></h2><p>Apply tensorflow trained model on android</p>
<p>说明<br>本文将描述如何将一个训练好的模型植入到android设备上，并且在android设备上输入待处理数据，通过模型，获取输出数据。 通过一个例子，讲述整个移植的过程。（demo的源码访问github上了<a href="https://github.com/CrystalChen1017/TSFOnAndroid）" target="_blank" rel="external">https://github.com/CrystalChen1017/TSFOnAndroid）</a> 整体的思路如下：</p>
<p>使用python在PC上训练好你的模型，保存为pb文件<br>新建android project，把pb文件放到assets文件夹下<br>将tensorflow的so文件以及jar包放到libs下<br>加载库文件，让tensorflow在app中运行起来<br>准备<br>tensorflow的环境，参阅<a href="http://blog.csdn.net/cxq234843654/article/details/70857562" target="_blank" rel="external">http://blog.csdn.net/cxq234843654/article/details/70857562</a><br>libtensorflow_inference.so<br>libandroid_tensorflow_inference_java.jar<br>如果要自己编译得到以上两个文件，需要安装bazel。参阅<a href="http://blog.csdn.net/cxq234843654/article/details/70861155" target="_blank" rel="external">http://blog.csdn.net/cxq234843654/article/details/70861155</a> 的第2步<br>以上两个文件通过以下两个网址进行下载： <a href="https://github.com/CrystalChen1017/TSFOnAndroid/tree/master/app/libs" target="_blank" rel="external">https://github.com/CrystalChen1017/TSFOnAndroid/tree/master/app/libs</a> 或者 <a href="http://download.csdn.net/detail/cxq234843654/9833372" target="_blank" rel="external">http://download.csdn.net/detail/cxq234843654/9833372</a></p>
<p>PC端模型的准备<br>这是一个很简单的模型，输入是一个数组matrix1，经过操作后，得到这个数组乘以2*matrix1。</p>
<p>给输入数据命名为input,在android端需要用这个input来为输入数据赋值<br>给输输数据命名为output,在android端需要用这个output来为获取输出的值<br>不能使用 tf.train.write_graph()保存模型，因为它只是保存了模型的结构，并不保存训练完毕的参数值<br>不能使用 tf.train.saver()保存模型，因为它只是保存了网络中的参数值，并不保存模型的结构。<br>graph_util.convert_variables_to_constants可以把整个sesion当作常量都保存下来，通过output_node_names参数来指定输出<br>tf.gfile.FastGFile(‘model/cxq.pb’, mode=’wb’)指定保存文件的路径以及读写方式<br>f.write（output_graph_def.SerializeToString()）将固化的模型写入到文件</p>
<p> -<em>- coding:utf-8 -</em>-<br>import tensorflow as tf<br>from tensorflow.python.client import graph_util</p>
<p>session = tf.Session()</p>
<p>matrix1 = tf.constant([[3., 3.]], name=’input’)<br>add2Mat = tf.add(matrix1, matrix1, name=’output’)</p>
<p>session.run(add2Mat)</p>
<p>output_graph_def = graph_util.convert_variables_to_constants(session, session.graph_def,output_node_names=[‘output’])</p>
<p>with tf.gfile.FastGFile(‘model/cxq.pb’, mode=’wb’) as f:<br>    f.write(output_graph_def.SerializeToString())</p>
<p>session.close()<br>运行后就会在model文件夹下产生一个cxq.pb文件，现在这个文件将刚才一系列的操作固化了，因此下次需要计算变量乘2时，我们可以直接拿到pb文件，指定输入，再获取输出。</p>
<p>（可选的）bazel编译出so和jar文件<br>如果希望自己通过tensorflow的源码编译出so和jar文件，则需要通过终端进入到tensorflow的目录下，进行如下操作：</p>
<p>编译so库<br>bazel build -c opt //tensorflow/contrib/android:libtensorflow_inference.so \<br>    – crosstool_top=//external:android/crosstool \<br>    – host_crosstool_top=@bazel_tools//tools/cpp:toolchain \<br>    – cpu=armeabi-v7a<br>编译完毕后，libtensorflow_inference.so的路径为：<br>/tensorflow/bazel-bin/tensorflow/contrib/android</p>
<p>编译jar包<br>bazel build //tensorflow/contrib/android:android_tensorflow_inference_java<br>编译完毕后，android_tensorflow_inference_java.jar的路径为：<br>/tensorflow/bazel-bin/tensorflow/contrib/android</p>
<p>android端的准备<br>新建一个Android Project<br>把刚才的pb文件存放到assets文件夹下<br>将libandroid_tensorflow_inference_java.jar存放到/app/libs目录下，并且右键“add as Libary”<br>在/app/libs下新建armeabi文件夹，并将libtensorflow_inference.so放进去<br>配置app:gradle以及gradle.properties<br>在android节点下添加soureSets，用于制定jniLibs的路径<br>sourceSets {<br>        main {<br>            jniLibs.srcDirs = [‘libs’]<br>        }<br>    }<br>在defaultConfig节点下添加<br>defaultConfig {</p>
<pre><code>    ndk {
        abiFilters &quot;armeabi&quot;
    }
}
</code></pre><p>在gradle.properties中添加下面一行<br>android.useDeprecatedNdk=true<br>通过以上3步操作，tensorflow的环境已经部署好了。</p>
<p>模型的调用<br>我们先新建一个MyTSF类，在这个类里面进行模型的调用，并且获取输出</p>
<p>package com.learn.tsfonandroid;</p>
<p>import android.content.res.AssetManager;<br>import android.os.Trace;</p>
<p>import org.tensorflow.contrib.android.TensorFlowInferenceInterface;</p>
<p>public class MyTSF {<br>    private static final String MODEL_FILE = “file:///android_asset/cxq.pb”; //模型存放路径</p>
<pre><code>//数据的维度
private static final int HEIGHT = 1;
private static final int WIDTH = 2;

//模型中输出变量的名称
private static final String inputName = &quot;input&quot;;
//用于存储的模型输入数据
private float[] inputs = new float[HEIGHT * WIDTH];

//模型中输出变量的名称
private static final String outputName = &quot;output&quot;;
//用于存储模型的输出数据
private float[] outputs = new float[HEIGHT * WIDTH];



TensorFlowInferenceInterface inferenceInterface;


static {
    //加载库文件
    System.loadLibrary(&quot;tensorflow_inference&quot;);
}

MyTSF(AssetManager assetManager) {
    //接口定义
    inferenceInterface = new TensorFlowInferenceInterface(assetManager,MODEL_FILE);
}

public float[] getAddResult() {
    //为输入数据赋值
    inputs[0]=1;
    inputs[1]=3;

    //将数据feed给tensorflow
    Trace.beginSection(&quot;feed&quot;);
    inferenceInterface.feed(inputName, inputs, WIDTH, HEIGHT);
    Trace.endSection();

    //运行乘2的操作
    Trace.beginSection(&quot;run&quot;);
    String[] outputNames = new String[] {outputName};
    inferenceInterface.run(outputNames);
    Trace.endSection();

    //将输出存放到outputs中
    Trace.beginSection(&quot;fetch&quot;);
    inferenceInterface.fetch(outputName, outputs);
    Trace.endSection();

    return outputs;
}
</code></pre><p>}</p>
<p>在Activity中使用MyTSF类</p>
<p> public void click01(View v){<br>        Log.i(TAG, “click01: “);<br>        MyTSF mytsf=new MyTSF(getAssets());<br>        float[] result=mytsf.getAddResult();<br>        for (int i=0;i&lt;result.length;i++){<br>            Log.i(TAG, “click01: “+result[i] );<br>        }</p>
<pre><code>}
</code></pre><h2 id="TensorFlow集成Android工程的框架"><a href="#TensorFlow集成Android工程的框架" class="headerlink" title="TensorFlow集成Android工程的框架"></a><a href="https://github.com/SpikeKing/TFAndroid" target="_blank" rel="external">TensorFlow集成Android工程的框架</a></h2><p>TFAndroid<br>TensorFlow集成Android工程的框架</p>
<p>框架细节，<a href="http://www.jianshu.com/p/870e9a54749a" target="_blank" rel="external">参考</a></p>
<p><a href="https://www.jianshu.com/p/870e9a54749a" target="_blank" rel="external">TensorFlow集成Android工程的框架</a><br>SpikeKing   2017.09.21</p>
<p>在Android工程中，集成TensorFlow模型。运行TensorFlow的默认Android工程，请参考。</p>
<p>Android源码：<a href="https://github.com/SpikeKing/TFAndroid/tree/master" target="_blank" rel="external">https://github.com/SpikeKing/TFAndroid/tree/master</a></p>
<p>库及模型的大小</p>
<p>libtensorflow_inference.so  10.2 M<br>libandroid_tensorflow_inference_java.jar  27 KB<br>optimized_tfdroid.pb  291 B</p>
<p>如果将so转换为jar库，<a href="https://www.jianshu.com/p/585be9d8db53" target="_blank" rel="external">参考</a>，则TF的so由10.2M缩小至4.1M。</p>
<p><img src="https://upload-images.jianshu.io/upload_images/749674-18941f05fd616658.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""><br>TF Android</p>
<p>TensorFlow<br>TF模型源码：<br><a href="https://github.com/SpikeKing/MachineLearningTutorial/blob/master/tests/android_test.py" target="_blank" rel="external">https://github.com/SpikeKing/MachineLearningTutorial/blob/master/tests/android_test.py</a></p>
<p>创建TensorFlow模型，简单的y=WX+b，存储图信息write_graph，存储参数信息saver.save。输入数据placeholder是I，输出数据是O。</p>
<p>import tensorflow as tf</p>
<p>I = tf.placeholder(tf.float32, shape=[None, 3], name=’I’)  # input<br>W = tf.Variable(tf.zeros(shape=[3, 2]), dtype=tf.float32, name=’W’)  # weights<br>b = tf.Variable(tf.zeros(shape=[2]), dtype=tf.float32, name=’b’)  # biases<br>O = tf.nn.relu(tf.matmul(I, W) + b, name=’O’)  # activation / output</p>
<p>saver = tf.train.Saver()<br>init_op = tf.global_variables_initializer()</p>
<p>with tf.Session() as sess:<br>    sess.run(init_op)</p>
<pre><code>tf.train.write_graph(sess.graph_def, &apos;./data/android/&apos;, &apos;tfdroid.pbtxt&apos;)  # 存储TensorFlow的图

# 训练数据，本例直接赋值
sess.run(tf.assign(W, [[1, 2], [4, 5], [7, 8]]))
sess.run(tf.assign(b, [1, 1]))

# 存储checkpoint文件，即参数信息
saver.save(sess, &apos;./data/android/tfdroid.ckpt&apos;)
</code></pre><p>创建Freeze的图，将图结构与参数组合在一起，生成模型，<a href="https://link.jianshu.com/?t=http://blog.csdn.net/czq7511/article/details/72452985" target="_blank" rel="external">参考</a>。</p>
<p>def gnr_freeze_graph(input_graph, input_saver, input_binary, input_checkpoint,<br>                     output_node_names, output_graph, clear_devices):<br>    “””<br>    将输入图与参数结合在一起</p>
<pre><code>:param input_graph: 输入图
:param input_saver: Saver解析器
:param input_binary: 输入图的格式，false是文本，true是二进制
:param input_checkpoint: checkpoint，检查点文件

:param output_node_names: 输出节点名称
:param output_graph: 保存输出文件
:param clear_devices: 清除训练设备
:return: NULL
&quot;&quot;&quot;
restore_op_name = &quot;save/restore_all&quot;
filename_tensor_name = &quot;save/Const:0&quot;

freeze_graph.freeze_graph(
    input_graph=input_graph,  # 输入图
    input_saver=input_saver,  # Saver解析器
    input_binary=input_binary,  # 输入图的格式，false是文本，true是二进制
    input_checkpoint=input_checkpoint,  # checkpoint，检查点文件
    output_node_names=output_node_names,  # 输出节点名称
    restore_op_name=restore_op_name,  # 从模型恢复节点的名字
    filename_tensor_name=filename_tensor_name,  # tensor名称
    output_graph=output_graph,  # 保存输出文件
    clear_devices=clear_devices,  # 清除训练设备
    initializer_nodes=&quot;&quot;)  # 初始化节点
</code></pre><p>优化模型，剪切节点，模型只保留输入输出的参数。</p>
<p>def gnr_optimize_graph(graph_path, optimized_graph_path):<br>    “””<br>    优化图<br>    :param graph_path: 原始图<br>    :param optimized_graph_path: 优化的图<br>    :return: NULL<br>    “””<br>    input_graph_def = tf.GraphDef()  # 读取原始图<br>    with tf.gfile.Open(graph_path, “r”) as f:<br>        data = f.read()<br>        input_graph_def.ParseFromString(data)</p>
<pre><code># 设置输入输出节点，剪切分支，大约节省1/4
output_graph_def = optimize_for_inference_lib.optimize_for_inference(
    input_graph_def,
    [&quot;I&quot;],  # an array of the input node(s)
    [&quot;O&quot;],  # an array of output nodes
    tf.float32.as_datatype_enum)

# 存储优化的图
f = tf.gfile.FastGFile(optimized_graph_path, &quot;w&quot;)
f.write(output_graph_def.SerializeToString())
</code></pre><p>执行函数，生成模型，frozen_tfdroid.pb和optimized_tfdroid.pb。</p>
<p>if <strong>name</strong> == “<strong>main</strong>“:<br>    input_graph_path = MODEL_FOLDER + MODEL_NAME + ‘.pbtxt’  # 输入图<br>    checkpoint_path = MODEL_FOLDER + MODEL_NAME + ‘.ckpt’  # 输入参数<br>    output_path = MODEL<em>FOLDER + ‘frozen</em>‘ + MODEL_NAME + ‘.pb’  # Freeze模型</p>
<pre><code>gnr_freeze_graph(input_graph=input_graph_path, input_saver=&quot;&quot;,
                 input_binary=False, input_checkpoint=checkpoint_path,
                 output_node_names=&quot;O&quot;, output_graph=output_path, clear_devices=True)

optimized_output_graph = MODEL_FOLDER + &apos;optimized_&apos; + MODEL_NAME + &apos;.pb&apos;

gnr_optimize_graph(output_path, optimized_output_graph)
</code></pre><p>Android<br>编译Android的库，<a href="https://link.jianshu.com/?t=https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/android" target="_blank" rel="external">参考</a>，或者，直接在Nightly中下载，<a href="https://link.jianshu.com/?t=https://ci.tensorflow.org/view/Nightly/job/nightly-android/44/artifact/" target="_blank" rel="external">参考</a>，archive.zip，大约158M。</p>
<p>创建Android工程，添加app/libs/中添加库文件。</p>
<p>armeabi-v7a/libtensorflow_inference.so<br>libandroid_tensorflow_inference_java.jar</p>
<p>在build.gradle中，添加</p>
<p>android {<br>    sourceSets {<br>        main {<br>            jniLibs.srcDirs = [‘libs’]<br>        }<br>    }<br>}<br>在app/src/main/assets中，添加模型optimized_tfdroid.pb文件。</p>
<p>在MainActivity中，添加so库。</p>
<p>static {<br>    System.loadLibrary(“tensorflow_inference”);<br>}</p>
<p>模型文件在assets中，TF的核心接口类TensorFlowInferenceInterface。</p>
<p>private static final String MODEL_FILE = “file:///android_asset/optimized_tfdroid.pb”;</p>
<p>private TensorFlowInferenceInterface mInferenceInterface;</p>
<p>初始模型文件</p>
<p>mInferenceInterface = new TensorFlowInferenceInterface();<br>mInferenceInterface.initializeTensorFlow(getAssets(), MODEL_FILE);</p>
<p>模型Feed数据，输入点名称是INPUT_NODE，输入结构INPUT_SIZE，输入数据inputFloats。</p>
<p>float[] inputFloats = {num1, num2, num3};<br>mInferenceInterface.fillNodeFloat(INPUT_NODE, INPUT_SIZE, inputFloats);<br>模型执行文件，输出点名称是OUTPUT_NODE，即”O”</p>
<p>mInferenceInterface.runInference(new String[]{OUTPUT_NODE});</p>
<p>输出数据结构</p>
<p>float[] resu = {0, 0};<br>mInferenceInterface.readNodeFloat(OUTPUT_NODE, resu);</p>
<p>最后，在layout中创建GUI布局。</p>
<p>效果</p>
<p><img src="https://upload-images.jianshu.io/upload_images/749674-0775a2794ea37717.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></p>
<p>TensorFlow集成至春雨医生</p>
<p><img src="https://upload-images.jianshu.io/upload_images/749674-81025bb04ad038a2.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/700" alt=""></p>
<p>That’s all! Enjoy it!</p>
<h2 id="AndroidTensorFlowMachineLearningExample"><a href="#AndroidTensorFlowMachineLearningExample" class="headerlink" title="AndroidTensorFlowMachineLearningExample"></a><a href="https://github.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample" target="_blank" rel="external">AndroidTensorFlowMachineLearningExample</a></h2><blockquote>
<p>Android TensorFlow MachineLearning Example (Building TensorFlow for Android)</p>
</blockquote>
<p><img src="http://opb37u589.bkt.clouddn.com/ml_android.png?imageMogr2/thumbnail/!75p" alt="ml_android.png"></p>
<p>About Android TensorFlow Machine Learning Example</p>
<p>This is an example project for integrating TensorFlow into Android application<br>How to build TensorFlow project to use with Android project.<br>How to build TensorFlow library(.so file and jar file) to use with Android Application.<br>This project include an example for object detection for an image taken from camera using TensorFlow library.</p>
<p><a href="https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc" target="_blank" rel="external">Read this article. It describes everything about building TensorFlow for Android.</a></p>
<p><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/keyboard_example.png" alt=""><br><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/pen_example.png" alt=""><br><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/wallet_example.png" alt=""></p>
<p><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMachineLearningExample/master/assets/sample_combined.png" alt=""></p>
<h2 id="Android-TensorFlow-MachineLearning-MNIST-Example-Building-Model-with-TensorFlow-for-Android"><a href="#Android-TensorFlow-MachineLearning-MNIST-Example-Building-Model-with-TensorFlow-for-Android" class="headerlink" title="Android TensorFlow MachineLearning MNIST Example (Building Model with TensorFlow for Android)"></a><a href="https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample" target="_blank" rel="external">Android TensorFlow MachineLearning MNIST Example (Building Model with TensorFlow for Android)</a></h2><p>About Android TensorFlow Machine Learning MNIST Example<br>This is an example project for creating machine learning model for MNIST to detect hand written digits.<br>Check this project for building tensorFlow for Android.<br>Now, there is no need to build the library as the it is now available through maven. Check this pull request.</p>
<p><a href="https://blog.mindorks.com/creating-custom-model-for-android-using-tensorflow-3f963d270bfb" target="_blank" rel="external">Read this article. It describes everything about creating custom model for Android using TensorFlow.</a></p>
<p><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/1.png" alt=""><br><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/2.png" alt=""><br><img src="https://raw.githubusercontent.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/master/assets/3.png" alt=""></p>
<blockquote>
<p>How to train model?</p>
</blockquote>
<p>To create model by yourself, install TensorFlow and run python scripts like</p>
<pre><code>$ python mnist.py
</code></pre><h2 id="HyperLPR"><a href="#HyperLPR" class="headerlink" title="HyperLPR"></a><a href="https://github.com/zeusees/HyperLPR" target="_blank" rel="external">HyperLPR</a></h2><p>基于深度学习高性能中文车牌识别 High Performance Chinese License Plate Recognition Framework. <a href="http://www.zeusee.com" target="_blank" rel="external">http://www.zeusee.com</a></p>
<p>High Accuracy Chinese License Plate Recognition Framework<br>介绍<br>This research aims at simply developping plate recognition project based on deep learning methods, with low complexity and high speed. This project has been used by some commercial corporations. Free and open source, deploying by Zeusee. this pipline also can apply to other countries license plate by training</p>
<p>HyperLPR是一个使用深度学习针对对中文车牌识别的实现，与其他开源的中文车牌识别框架相比，它的检测速度和鲁棒性和多场景的适应性都要好于其他的开源框架。</p>
<p>相关资源<br>在线测试地址(已失效)。<br>相关技术博客(技术文章会在接下来的几个月的时间内连续更新)。<br>带UI界面的工程(感谢群内小伙伴的工作)。<br>端到端(多标签分类)训练代码(感谢群内小伙伴的工作)。<br>端到端(CTC)训练代码(感谢群内小伙伴工作)。<br>训练代码和字符分割介绍<br>IOS版本 xiaojun123456贡献<br>更新<br>增加了端到端模型的cpp实现,识别速度比分割快30%(Linux)(2018.1.31)<br>增加字符分割训练代码和字符分割介绍(2018.1.)<br>更新了Android实现，大幅提高准确率和速度 (骁龙835 (720x1280) ~50ms )(2017.12.27)<br>添加了IOS版本的实现（感谢xiaojun123456的工作）<br>添加端到端的序列识别模型识别率大幅度提升,使得无需分割字符即可识别,识别速度提高20% (2017.11.17)<br>新增的端到端模型可以识别新能源车牌、教练车牌、白色警用车牌、武警车牌 (2017.11.17)<br>更新Windows版本的Visual Studio 2015 工程（2017.11.15）<br>增加cpp版本,目前仅支持标准蓝牌(需要依赖OpenCV 3.3) (2017.10.28)<br>TODO<br>提供字符字符识别的训练代码<br>改进精定位方法<br>C++版的端到端识别模型<br>特性<br>速度快 720p ，单核 Intel 2.2G CPU (macbook Pro 2015)平均识别时间低于100ms<br>基于端到端的车牌识别无需进行字符分割<br>识别率高,仅仅针对车牌ROI在EasyPR数据集上，0-error达到 95.2%, 1-error识别率达到 97.4% (指在定位成功后的车牌识别率)<br>轻量 总代码量不超1k行<br>注意事项:<br>Win工程中若需要使用静态库，需单独编译<br>本项目的C++实现和Python实现无任何关联，都为单独实现<br>在编译C++工程的时候必须要使用OpenCV 3.3(DNN 库)，否则无法编译<br>Python 依赖<br>Keras (&gt;2.0.0)<br>Theano(&gt;0.9) or Tensorflow(&gt;1.1.x)<br>Numpy (&gt;1.10)<br>Scipy (0.19.1)<br>OpenCV(&gt;3.0)<br>Scikit-image (0.13.0)<br>PIL<br>CPP 依赖<br>Opencv 3.3<br>简单使用方式<br>from hyperlpr import  pipline as  pp<br>import cv2<br>image = cv2.imread(“filename”)<br>image,res  = pp.SimpleRecognizePlate(image)<br>print(res)<br>Linux/Mac 编译<br>仅需要的依赖OpenCV 3.3 (需要DNN框架)<br>cd cpp_implementation<br>mkdir build<br>cd build<br>cmake ../<br>sudo make -j<br>CPP demo</p>
<p>#include “../include/Pipeline.h”<br>int main(){<br>    pr::PipelinePR prc(“model/cascade.xml”,<br>                      “model/HorizonalFinemapping.prototxt”,”model/HorizonalFinemapping.caffemodel”,<br>                      “model/Segmentation.prototxt”,”model/Segmentation.caffemodel”,<br>                      “model/CharacterRecognization.prototxt”,”model/CharacterRecognization.caffemodel”,<br>                       “model/SegmentationFree.prototxt”,”model/SegmentationFree.caffemodel”<br>                    );<br>  //定义模型文件</p>
<pre><code>cv::Mat image = cv::imread(&quot;/Users/yujinke/ClionProjects/cpp_ocr_demo/test.png&quot;);
std::vector&lt;pr::PlateInfo&gt; res = prc.RunPiplineAsImage(image,pr::SEGMENTATION_FREE_METHOD);
</code></pre><p>  //使用端到端模型模型进行识别 识别结果将会保存在res里面</p>
<pre><code>for(auto st:res) {
    if(st.confidence&gt;0.75) {
        std::cout &lt;&lt; st.getPlateName() &lt;&lt; &quot; &quot; &lt;&lt; st.confidence &lt;&lt; std::endl;
      //输出识别结果 、识别置信度
        cv::Rect region = st.getPlateRect();
      //获取车牌位置
</code></pre><p> cv::rectangle(image,cv::Point(region.x,region.y),cv::Point(region.x+region.width,region.y+region.height),cv::Scalar(255,255,0),2);<br>          //画出车牌位置</p>
<pre><code>    }
}

cv::imshow(&quot;image&quot;,image);
cv::waitKey(0);
return 0 ;
</code></pre><p>}<br>可识别和待支持的车牌的类型<br> 单行蓝牌<br> 单行黄牌<br> 新能源车牌<br> 白色警用车牌<br> 使馆/港澳车牌<br> 教练车牌<br> 武警车牌<br> 民航车牌<br> 双层黄牌<br> 双层武警<br> 双层军牌<br> 双层农用车牌<br> 双层个性化车牌<br>Note:由于训练的时候样本存在一些不均衡的问题,一些特殊车牌存在一定识别率低下的问题，如(使馆/港澳车牌)，会在后续的版本进行改进。<br>测试样例</p>
<p><img src="https://github.com/zeusees/HyperLPR/blob/master/demo_images/test.png" alt=""></p>
<p><img src="https://github.com/zeusees/HyperLPR/raw/master/demo_images/15.jpg" alt=""></p>
<p>Android示例</p>
<p><img src="https://github.com/zeusees/HyperLPR/raw/master/demo_images/android.png" alt=""></p>
<p>识别测试APP<br>体验 Android APP：<a href="https://fir.im/HyperLPR" target="_blank" rel="external">https://fir.im/HyperLPR</a> (根据图片尺寸调整程序中的尺度，提高准确率)<br>感谢 sundyCoder Android 版本完善</p>
<h2 id="hyperlpr4Android"><a href="#hyperlpr4Android" class="headerlink" title="hyperlpr4Android"></a><a href="https://github.com/sundyCoder/hyperlpr4Android" target="_blank" rel="external">hyperlpr4Android</a></h2><p>hyperlpr4Android<br>热烈欢迎感兴趣的朋友，一起加入Android平台的研发。</p>
<p>参见 <a href="https://github.com/zeusees/HyperLPR" target="_blank" rel="external">https://github.com/zeusees/HyperLPR</a></p>
<p>效果如下:</p>
<p><img src="https://github.com/sundyCoder/hyperlpr4Android/raw/master/hyperlpr.jpg" alt=""></p>
<h2 id="mnist-android-tensorflow"><a href="#mnist-android-tensorflow" class="headerlink" title="mnist-android-tensorflow"></a><a href="https://github.com/mari-linhares/mnist-android-tensorflow" target="_blank" rel="external">mnist-android-tensorflow</a></h2><p>MNIST on Android with TensorFlow<br>This work was featured on <a href="https://www.youtube.com/watch?v=kFWKdLOxykE&amp;t=67s" target="_blank" rel="external">a video from Siraj Raval!</a></p>
<p><img src="https://camo.githubusercontent.com/253d86d976cfae14e1b02f02e30179fc0d6b01d7/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6b46574b644c4f78796b452f687164656661756c742e6a7067" alt=""></p>
<p>Check the video demo <a href="https://www.youtube.com/watch?v=gahi0Hjgokw" target="_blank" rel="external">here</a>.</p>
<p><img src="https://github.com/mari-linhares/mnist-android-tensorflow/raw/master/images/demo.png" alt=""></p>
<p>Beautiful art work, right? I know.</p>
<p>Handwritten digits classification from MNIST on Android with TensorFlow.</p>
<p>If you want to make your own version of this app or want to know how to save your model and export it for Android or other devices check the very simple tutorial below.</p>
<p>The UI and expert-graph.pb model were taken from: <a href="https://github.com/miyosuda/TensorFlowAndroidMNIST" target="_blank" rel="external">https://github.com/miyosuda/TensorFlowAndroidMNIST</a>, so thank you miyousuda.<br>The TensorFlow jar and so armeabi-v7a were taken from: <a href="https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample" target="_blank" rel="external">https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample</a>, so thank you MindorksOpenSource.<br>The Tensorflow so of x86 was taken from: <a href="https://github.com/cesardelgadof/TensorFlowAndroidMNIST" target="_blank" rel="external">https://github.com/cesardelgadof/TensorFlowAndroidMNIST</a>, so thank you cesardelgadof.</p>
<blockquote>
<p>How to run this?</p>
</blockquote>
<p>Just open this project with Android Studio and is ready to run, this will work with x86 and armeabi-v7a architectures.</p>
<blockquote>
<p>How to export my model?</p>
</blockquote>
<p>A full example can be seen <a href="https://github.com/mari-linhares/mnist-android-tensorflow/blob/master/tensorflow_model/convnet.py" target="_blank" rel="external">here</a></p>
<p>1.Train your model</p>
<p>2.Keep an in memory copy of eveything your model learned (like biases and weights) Example: _w = sess.eval(w), where w was learned from training.</p>
<p>3.Rewrite your model changing the variables for constants with value = in memory copy of learned variables. Example: w_save = tf.constant(_w)</p>
<p>Also make sure to put names in the input and output of the model, this will be needed for the model later. Example:<br>x = tf.placeholder(tf.float32, [None, 1000], name=’input’)<br>y = tf.nn.softmax(tf.matmul(x, w_save) + b_save), name=’output’)</p>
<p>4.Export your model with:<br>tf.train.write_graph(<graph>, <path for="" the="" exported="" model=""></path>, <name of="" the="" model="">.pb, as_text=False)</name></graph></p>
<blockquote>
<p>How to run my model with Android?</p>
</blockquote>
<p>You basically need two things:</p>
<p><a href="https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/blob/master/app/libs/libandroid_tensorflow_inference_java.jar" target="_blank" rel="external">The TensorFlow jar</a><br>Move it to the libs folder, right click and add as library.</p>
<p>The TensorFlow so file for the desired architecture:<br><a href="https://github.com/cesardelgadof/TensorFlowAndroidMNIST/blob/master/app/src/main/jniLibs/x86/libtensorflow_mnist.so" target="_blank" rel="external">x86</a><br><a href="https://github.com/MindorksOpenSource/AndroidTensorFlowMNISTExample/tree/master/app/src/main/jniLibs/armeabi-v7a" target="_blank" rel="external">armeabi-v7a</a></p>
<p>Creat the jniLibs/x86 folder or the jniLibs/armeabi-v7a folder at the main folder.<br>Move it to app/src/main/jniLibs/x86/libtensorflow_inference.so or app/src/jniLibs/armeabi-v7a/libtensorflow_inference.so</p>
<p>If you want to generate these files yourself, <a href="https://blog.mindorks.com/android-tensorflow-machine-learning-example-ff0e9b2654cc" target="_blank" rel="external">here</a> is a nice tutorial of how to do it.</p>
<blockquote>
<p>Interacting with TensorFlow</p>
</blockquote>
<p>To interact with TensorFlow you will need an instance of TensorFlowInferenceInterface, you can see more details about it <a href="https://github.com/mari-linhares/mnist-android-tensorflow/blob/master/MnistAndroid/app/src/main/java/mariannelinhares/mnistandroid/Classifier.java" target="_blank" rel="external">here</a></p>
<p>Thank you, have fun!</p>
<h2 id="android-yolo"><a href="#android-yolo" class="headerlink" title="android-yolo"></a><a href="https://github.com/natanielruiz/android-yolo" target="_blank" rel="external">android-yolo</a></h2><p>Real-time object detection on Android using the YOLO network with TensorFlow</p>
<p>TensorFlow YOLO object detection on Android</p>
<p><img src="https://camo.githubusercontent.com/fcdf8f032e14ba941e9e387c87ff0c5cb835113b/687474703a2f2f692e696d6775722e636f6d2f68736b64766f692e706e67" alt=""></p>
<p><a href="https://github.com/miyosuda/TensorFlowAndroidDemo" target="_blank" rel="external">Source project</a></p>
<p>android-yolo is the first implementation of YOLO for TensorFlow on an Android device. It is compatible with Android Studio and usable out of the box. It can detect the 20 classes of objects in the Pascal VOC dataset: aeroplane, bicycle, bird, boat, bottle, bus, car, cat, chair, cow, dining table, dog, horse, motorbike, person, potted plant, sheep, sofa, train and tv/monitor. The network only outputs one predicted bounding box at a time for now. The code can and will be extended in the future to output several predictions.</p>
<p>To use this demo first clone the repository. Download the TensorFlow <a href="https://drive.google.com/file/d/0B2fFW2t9-qW3MVJlQ29LRzlLT2c/view?usp=sharing" target="_blank" rel="external">YOLO model </a>and put it in android-yolo/app/src/main/assets. Then open the project on Android Studio. Once the project is open you can run the project on your Android device using the Run ‘app’ command and selecting your device.</p>
<p>NEW: The standalone APK has been released and you can find it <a href="https://drive.google.com/open?id=0B2fFW2t9-qW3LWFDNXVHUE9rV3M" target="_blank" rel="external">here</a>. Just open your browser on your Android device and download the APK file. When the file has been downloaded it should begin installing on your device after you grant the required permissions.</p>
<p>GPUs are not currently supported by TensorFlow on Android. If you have a decent Android device you will have around two frames per second of processed images.</p>
<p>Here is a <a href="http://youtu.be/EhMrf4G5Wf0" target="_blank" rel="external">video</a> showing a small demo of the app.</p>
<p>Nataniel Ruiz<br>School of Interactive Computing<br>Georgia Institute of Technology</p>
<p>Credits: App launch icon made by Freepik from Flaticon is licensed by Creative Commons BY 3.0.</p>
<p>Disclaimer: The app is hardcoded for 20 classes and for the tiny-yolo network final output layer. You can check the following code if you want to change this:</p>
<p><a href="https://github.com/natanielruiz/android-yolo/blob/master/app/src/main/java/org/tensorflow/demo/TensorflowClassifier.java" target="_blank" rel="external">https://github.com/natanielruiz/android-yolo/blob/master/app/src/main/java/org/tensorflow/demo/TensorflowClassifier.java</a></p>
<p>The code describes the interpretation of the output.</p>
<p>The code for the network inference pass is written in C++ and the output is passed to Java. The output of the network is in the form of a String which is converted to a StringTokenizer and is then converted into an array of Floats in line 87 of TensorflowClassifier.java</p>
<p>You can work from there and read the papers to transform the new yolo model output into something that makes sense. (I did it only for one bounding box and also obtained the confidence of this bounding box). This part of the code is commented by me so you can understand what I did. Also read the paper here: <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="external">https://arxiv.org/abs/1506.02640</a></p>
<h2 id="yolo-tiny-v1-mobile"><a href="#yolo-tiny-v1-mobile" class="headerlink" title="yolo-tiny-v1-mobile"></a><a href="https://github.com/mdietrichstein/yolo-tiny-v1-mobile" target="_blank" rel="external">yolo-tiny-v1-mobile</a></h2><p>Yolo for Android and iOS - Mobile Deep Learning Object Detection in Realtime</p>
<p>olo for Android and iOS - Mobile Deep Learning Object Detection in Realtime<br>This repository contains an implementation of the (Tiny) Yolo V1 objector detector for both Android and iOS.</p>
<p>Yolo?<br>This Ted Talk by the creator of Yolo itself gives a nice high-level overview: Joseph Redmon - How a computer learns to recognize objects instantly.</p>
<p>You should also check out his CV. Really, do it ;)</p>
<p>The notebooks provided in this repository contain some more references regarding Yolo.</p>
<p>Motivation<br>This project came to be because I wanted to apply the knowledge I have gained from various Deep Learning related courses over the year in a practical project and was searching for a workflow which supports:</p>
<p>Model Exploration/Implementation<br>Model Training/Validation<br>Model Optimization<br>Deployment on iOS and Android<br>Features<br>Realtime object detection<br>Support for Android and iOS<br>“Live” Switching between Portrait and Landscape Orientation<br>Prerequisites<br>Jupyter Notebooks<br>The notebooks should be compatible with Python 3.5, Keras 2, Tensorflow 1.2.x. You can find the complete list of dependencies in environment.yml</p>
<p>Android<br>The Android app is written in Kotlin and should work with any Android Studio Version from 3.x onwards.</p>
<p>iOS<br>Run pod install to install the required dependencies via Cocoapods. The iOS app is written in Swift 3 and Object C++ and should work with a recent version of Xcode.</p>
<p>Build Process</p>
<ol>
<li><p>Create notebooks/tf-exports folder<br>The notebooks will use this folder to export models to.</p>
</li>
<li><p>Follow the instructions in notebooks/01_exploration.ipynb to create a keras model with tensorflow backend<br>This notebook documents the process of implementing Yolo in Keras, converting the pretrained darknet weights for keras and converting them to a format compatible with the tensorflow backend.</p>
</li>
<li><p>Follow the instructions in notebooks/02_export_to_tf.ipynb to export an optimized tensorflow model<br>This notebook shows how to export the keras model to tensorflow and how to optimize it for inference. The resulting model files contain the tensorflow model that will be loaded by the mobile apps.</p>
</li>
<li><p>Include model file in mobile projects<br>iOS: Open the project in XCode and drag and drop frozen_yolo.pb into XCode.</p>
</li>
</ol>
<p>Android: Create a folder named mobile/Android/YoloTinyV1Tensorflow/app/src/main/assets and copy optimized_yolo.pb into it.</p>
<p>Note: You could try to use optimized_yolo.pb with iOS as well. It didn’t work with the version of tensorflow I was using though.</p>
<p>Improvements<br>Overlapping Detection Boxes<br>The mobile apps do not use Non-Maximum Suppression yet. This means that the apps will display multiple boxes for the same object. I will add this feature to the apps soon. Check out notebooks/01_exploration.ipynb if you’re interested in how this works, or you want to implement it youself.</p>
<p>Performance<br>Performance on Android and iOS is suboptimal. There are some opportunities to improve performance (e.g. weight quantization). Will definitely look into this some more.</p>
<p>Camera Switching<br>Both apps only use the back camera. A camera switcher would be a nice improvement.</p>
<h2 id="TensorFlowAndroidDemo"><a href="#TensorFlowAndroidDemo" class="headerlink" title="TensorFlowAndroidDemo"></a><a href="https://github.com/miyosuda/TensorFlowAndroidDemo" target="_blank" rel="external">TensorFlowAndroidDemo</a></h2><p>TensorFlow Android stand-alone demo</p>
<p>TensorFlow Android stand-alone demo<br>Android demo source files extracted from original TensorFlow source. (TensorFlow r0.10)</p>
<p>To build this demo, you don’t need to prepare build environment with Bazel, and it only requires AndroidStudio.</p>
<p>If you would like to build jni codes, only NDK is requied to build it.</p>
<p><img src="https://camo.githubusercontent.com/c7524a5212d3212f0c017a50cac6c765e1389964/687474703a2f2f6e6172722e6a702f707269766174652f6d69796f7368692f74656e736f72666c6f772f74656e736f72666c6f775f73637265656e312e706e67" alt=""></p>
<blockquote>
<p>How to build jni codes</p>
</blockquote>
<p>First install NDK, and set path for NDK tools, and then type commands below to create .so file.</p>
<p>$ cd jni-build<br>$ make<br>$ make install</p>
<h2 id="Tensorflow-MNIST-demo-on-Android"><a href="#Tensorflow-MNIST-demo-on-Android" class="headerlink" title="Tensorflow MNIST demo on Android"></a><a href="https://github.com/miyosuda/TensorFlowAndroidMNIST" target="_blank" rel="external">Tensorflow MNIST demo on Android</a></h2><p>TensorFlowAndroidMNIST - Android MNIST demo with TensorFlow<br>This is a demo app for Android with Tensorflow to detect handwritten digits.</p>
<p><img src="https://camo.githubusercontent.com/3a15c68b040d69a6a870c6207b838fce313aff0a/687474703a2f2f6e6172722e6a702f707269766174652f6d69796f7368692f74656e736f72666c6f772f6d6e6973745f73637265656e302e706e67" alt=""></p>
<p>This Android demo is based on Tensorflow tutorial.</p>
<p>MNIST For ML Beginners <a href="https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.10/tutorials/mnist/beginners/index.html</a></p>
<p>Deep MNIST for Experts <a href="https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html" target="_blank" rel="external">https://www.tensorflow.org/versions/r0.10/tutorials/mnist/pros/index.html</a></p>
<p>How to train model.<br>Training scripts for neural network model are located at</p>
<p><a href="https://github.com/miyosuda/TensorFlowAndroidMNIST/tree/master/trainer-script" target="_blank" rel="external">https://github.com/miyosuda/TensorFlowAndroidMNIST/tree/master/trainer-script</a></p>
<p>To create model by yourself, install Tensorflow and run python scripts like</p>
<p>$ python beginner.py<br>or</p>
<p>$ python expert.py<br>and locate exported .pb file to assets dir.</p>
<p>To export training model, I added some modification to original tutorial scripts.</p>
<p>Now Tensorflow cannot export network graph and trained network weight Variable at the same time, so we need to create another graph to export and convert Variable into constants.</p>
<p>After training is finished, converted trained Variable to numpy ndarray.</p>
<p>_W = W.eval(sess)<br>_b = b.eval(sess)<br>and then convert them into constant and re-create graph for exporting.</p>
<p>W_2 = tf.constant(_W, name=”constant_W”)<br>b_2 = tf.constant(_b, name=”constant_b”)<br>And then use tf.train.write_graph to export graph with trained weights.</p>
<p>How to build JNI codes<br>Native .so files are already built in this project, but if you would like to build it by yourself, please install and setup NDK.</p>
<p>First download, extract and place Android NDK.</p>
<p><a href="http://developer.android.com/intl/ja/ndk/downloads/index.html" target="_blank" rel="external">http://developer.android.com/intl/ja/ndk/downloads/index.html</a></p>
<p>And then update your PATH environment variable. For example,</p>
<p>export NDK_HOME=”/Users/[your-username]/Development/android/android-ndk-r11b”<br>export PATH=$PATH:$NDK_HOME<br>And build .so file in jni-build dir.</p>
<p>$ cd jni-build<br>$ make<br>and copy .so file into app/src/main/jniLibs/armeabi-v7a/ with</p>
<p>$ make install<br>(Unlike original Android demo in Tensorflow, you don’t need to install bazel to build this demo.</p>
<p>Tensorflow library files (.a files) and header files are extracted from original Tensorflow Android demo r0.10.</p>
<h2 id="Human-Activity-Recognition-using-CNN"><a href="#Human-Activity-Recognition-using-CNN" class="headerlink" title="Human-Activity-Recognition-using-CNN"></a><a href="https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN" target="_blank" rel="external">Human-Activity-Recognition-using-CNN</a></h2><p>CNN for Human Activity Recognition<br><a href="https://github.com/aqibsaeed/Human-Activity-Recognition-using-CNN/blob/master/Activity%20Detection.ipynb" target="_blank" rel="external">Python notebook</a> for blog post <a href="http://aqibsaeed.github.io/2016-11-04-human-activity-recognition-cnn/" target="_blank" rel="external">Implementing a CNN for Human Activity Recognition in Tensorflow</a>.</p>
<p>Tools Required<br>Python 2.7 is used during development and following libraries are required to run the code provided in the notebook:</p>
<p>Tensorflow<br>Numpy<br>Matplotlib<br>Pandas<br>Dataset<br>The WISDM Actitracker dataset used for model training, can be downloaded from the following <a href="http://www.cis.fordham.edu/wisdm/dataset.php" target="_blank" rel="external">link</a></p>
<p>Related Problem<br>User identification from walking activity. Accelerometer dataset from 22 indivduals can be downloaded from the following <a href="http://archive.ics.uci.edu/ml/datasets/User+Identification+From+Walking+Activity" target="_blank" rel="external">link</a></p>
<h2 id="A-Guide-to-Running-Tensorflow-Models-on-Android"><a href="#A-Guide-to-Running-Tensorflow-Models-on-Android" class="headerlink" title="A_Guide_to_Running_Tensorflow_Models_on_Android"></a><a href="https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android" target="_blank" rel="external">A_Guide_to_Running_Tensorflow_Models_on_Android</a></h2><p>This is the code for”A Guide to Running Tensorflow Models on Android” By SIraj Raval on Youtube</p>
<p>Overview<br>This is the code for this<a href="https://youtu.be/kFWKdLOxykE" target="_blank" rel="external"> video</a> on Youtube by Siraj Raval.</p>
<p><img src="https://github.com/llSourcell/A_Guide_to_Running_Tensorflow_Models_on_Android/raw/master/images/demo.png" alt=""></p>
<p>Handwritten digits classification from MNIST on Android with TensorFlow.</p>
<p>If you want to make your own version of this app or want to knowhow to save your model and export it for Android or other devices check the very simple tutorial below. The UI and expert-graph.pb model were taken from: <a href="https://github.com/miyosuda/TensorFlowAndroidMNIST" target="_blank" rel="external">https://github.com/miyosuda/TensorFlowAndroidMNIST</a>, so thank you miyousuda.</p>
<p>Dependencies<br>All included</p>
<p>Usage<br>Just open this project with Android Studio and is ready to run, this will work with x86 and armeabi-v7a architectures.</p>
<p>How to export my model?<br>A full example can be seen <a href="https://github.com/mari-linhares/mnist-android-tensorflow/blob/master/tensorflow_model/convnet.py" target="_blank" rel="external">here</a></p>
<p>Train your model</p>
<p>Keep an in memory copy of eveything your model learned (like biases and weights) Example: _w = sess.eval(w), where w was learned from training.</p>
<p>Rewrite your model changing the variables for constants with value = in memory copy of learned variables. Example: w_save = tf.constant(_w)</p>
<p>Also make sure to put names in the input and output of the model, this will be needed for the model later. Example:<br>x = tf.placeholder(tf.float32, [None, 1000], name=’input’)<br>y = tf.nn.softmax(tf.matmul(x, w_save) + b_save), name=’output’)</p>
<p>Export your model with:<br>tf.train.write_graph(<graph>, <path for="" the="" exported="" model=""></path>, <name of="" the="" model="">.pb, as_text=False)</name></graph></p>
<p>How to run my model with Android?<br>You need tensorflow.aar, which can be downloaded from <a href="http://ci.tensorflow.org/view/Nightly/job/nightly-android/" target="_blank" rel="external">the nightly build artifact of TensorFlow CI</a>, here we use <a href="http://ci.tensorflow.org/view/Nightly/job/nightly-android/124/artifact/" target="_blank" rel="external">the #124 build</a>.</p>
<p>Interacting with TensorFlow<br>To interact with TensorFlow you will need an instance of TensorFlowInferenceInterface, you can see more details about it <a href="https://github.com/mari-linhares/mnist-android-tensorflow/blob/master/MnistAndroid/app/src/main/java/mariannelinhares/mnistandroid/Classifier.java" target="_blank" rel="external">here</a></p>
<h2 id="TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"><a href="#TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" class="headerlink" title="TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"></a><a href="https://github.com/curiousily/TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs" target="_blank" rel="external">TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs</a></h2><p>iPython notebook and Android app that shows how to build LSTM model in TensorFlow and deploy it on Android</p>
<p>TensorFlow on Android for Human Activity Recognition with LSTMs<br>Full explanation can be found in <a href="https://medium.com/@curiousily/human-activity-recognition-using-lstms-on-android-tensorflow-for-hackers-part-vi-492da5adef64" target="_blank" rel="external">this blog post</a>. The source code is compatible with TensorFlow 1.1</p>
<h2 id="Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras"><a href="#Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras" class="headerlink" title="Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras"></a><a href="https://github.com/curiousily/Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras" target="_blank" rel="external">Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras</a></h2><p>Credit Card Fraud Detection using Autoencoders in Keras<br>Full explanation can be found in <a href="https://medium.com/@curiousily/credit-card-fraud-detection-using-autoencoders-in-keras-tensorflow-for-hackers-part-vii-20e0c85301bd" target="_blank" rel="external">this blog post</a>. The source code is compatible with TensorFlow 1.1 and Keras 2.0</p>
<h2 id="Real-Time-Semantic-Segmentation-in-Mobile-device"><a href="#Real-Time-Semantic-Segmentation-in-Mobile-device" class="headerlink" title="Real-Time Semantic Segmentation in Mobile device"></a><a href="https://github.com/akirasosa/mobile-semantic-segmentation" target="_blank" rel="external">Real-Time Semantic Segmentation in Mobile device</a></h2><p>Real-Time Semantic Segmentation in Mobile device<br>This project is an example project of semantic segmentation for mobile real-time app.</p>
<p>The architecture is inspired by MobileNets and U-Net.</p>
<p>LFW, Labeled Faces in the Wild, is used as a Dataset.</p>
<p>The goal of this project is to detect hair segments with reasonable accuracy and speed in mobile device. Currently, it achieves 0.89 IoU.</p>
<p>About speed vs accuracy, more details are available at my post.</p>
<p><img src="https://github.com/akirasosa/mobile-semantic-segmentation/raw/master/assets/prediction.png" alt=""></p>
<p>Example application<br>iOS<br>Android (TODO)<br>Requirements<br>Keras 2<br>TensorFlow as a backend of Keras and for Android app.<br>CoreML for iOS app.<br>About Model<br>At this time, there is only one model in this repository, MobileUNet.py. As a typical U-Net architecture, it has encoder and decoder parts, which consist of depthwise conv blocks proposed by MobileNets.</p>
<p>Input image is encoded to 1/32 size, and then decoded to 1/2. Finally, it scores the results and make it to original size.</p>
<p>Beside the U-Net like model, PSPNet like model was also tried. But it did not make a good result. Probably, global context does not have so much importance in the problem of hair recognition.</p>
<p>Steps to training<br>Data Preparation<br>Data is available at LFW. To get mask images, refer issue #11 for more. After you got images and masks, put the images of faces and masks as shown below.</p>
<p>data/<br>  raw/<br>    images/<br>      0001.jpg<br>      0002.jpg<br>    masks/<br>      0001.ppm<br>      0002.ppm<br>Then, convert it to numpy binary format for portability.</p>
<p>python data.py –img_size=128<br>Data augmentation will be done on the fly during training phase. I used rotation, shear ,zoom and horizontal flip.</p>
<p>Training<br>This repository contains three kinds of training scripts, transfer learning, fine tuning and full training. MobileNets is so compact that it’s possible to try full training in a short time.</p>
<p> Full training<br>python train_full.py \<br>  –img_file=/path/to/images.npy \<br>  –mask_file=/path/to/masks.npy<br>Dice coefficient is used as a loss function. Some other metrics are used such as precision, recall and binary cross entropy. Loss can be decreased soon smoothly even with high learning rate.</p>
<p>I also tried adding aux loss by using the segment of face part. Though, still I have not fully examined the effect of it, there maybe a little improvement of accuracy without dropping inference speed.</p>
<p>Converting<br>As the purpose of this project is to make model run in mobile device, this repository contains some scripts to convert models for iOS and Android.</p>
<p>coreml-converter.py<br>It converts trained hdf5 model to CoreML model for iOS app.<br>coreml-converter-bench.py<br>It generates non-trained CoreML model. It’s useful to measure the inference speed in iOS device.<br>tf-converter.py<br>It converts trained hdf5 model to protocol buffer format for TensorFlow which is used in Android app.<br>TBD<br> Report speed vs accuracy in mobile device.<br> Example app for Android<br> Aux loss<br> Some more optimizations??</p>
<h2 id="NewFeelings"><a href="#NewFeelings" class="headerlink" title="NewFeelings"></a><a href="https://github.com/dongchangzhang/NewFeelings" target="_blank" rel="external">NewFeelings</a></h2><p> A smart album for Android which use tensorflow to classify images</p>
<p> <img src="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/start.gif" alt=""></p>
<p> <img src="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/rcn2.gif" alt=""></p>
<p> <img src="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/rcn1.gif" alt=""></p>
<p> <img src="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/view.gif" alt=""></p>
<p> <img src="https://github.com/dongchangzhang/NewFeelings/raw/master/preview/take-photo.gif" alt=""></p>
<p> <a href="https://github.com/gaohuangzhang/Album-Category" target="_blank" rel="external">visit history of project：Album-Category</a></p>
<h2 id="tensorflow-classifier-android"><a href="#tensorflow-classifier-android" class="headerlink" title="tensorflow-classifier-android"></a><a href="https://github.com/Nilhcem/tensorflow-classifier-android" target="_blank" rel="external">tensorflow-classifier-android</a></h2><p> TensorFlow (1.4.0) Image Classifier Gradle Standalone Port<br>Clone the project, and checkout the tag 1.4.0<br>Import it on Android Studio<br>Run it<br>That’s all.<br>This project is a way to get started with TensorFlow Image Classifier quickly.</p>
<p>I am not planning to maintain it. If you need an updated version, build it yourself using hints from <a href="http://nilhcem.com/android/custom-tensorflow-classifier" target="_blank" rel="external">this blog post</a>.</p>
<p>Native libraries<br>Native compiled libraries are embedded in the 1.4.0 tag, so you won’t need to install the NDK.<br>However, this means that you cannot change the org.tensorflow.demo.env.ImageUtils class.<br>Here’s what you need to do if you want, for example, to use a different package name:</p>
<p>Install the <a href="https://developer.android.com/studio/projects/add-native-code.html" target="_blank" rel="external">NDK and build tools</a><br>Checkout the 1.4.0-cmake tag<br>Modify line 7 of the app/src/main/cpp/imageutils_jni.cpp file to specify your new package name</p>
<h2 id="ck-tensorflow"><a href="#ck-tensorflow" class="headerlink" title="ck-tensorflow"></a><a href="https://github.com/ctuning/ck-tensorflow" target="_blank" rel="external">ck-tensorflow</a></h2><p>Integration of TensorFlow to Collective Knowledge workflow framework to provide unified CK JSON API for AI (customized builds across diverse libraries and hardware, unified AI API, collaborative experiments, performance optimization and model/data set tuning): <a href="http://cKnowledge.org/ai" target="_blank" rel="external">http://cKnowledge.org/ai</a></p>
<h2 id="TFDroid"><a href="#TFDroid" class="headerlink" title="TFDroid"></a><a href="https://github.com/omimo/TFDroid" target="_blank" rel="external">TFDroid</a></h2><p>A simple demo for using Tensorflow models in Android apps <a href="https://omid.al/posts/2017-02-20-Tuto…" target="_blank" rel="external">https://omid.al/posts/2017-02-20-Tuto…</a></p>
<h2 id="ID-Card-with-TensorFlow-Opencv-in-Android"><a href="#ID-Card-with-TensorFlow-Opencv-in-Android" class="headerlink" title="ID-Card_with_TensorFlow_Opencv_in_Android"></a><a href="https://github.com/FishermanZzhang/ID-Card_with_TensorFlow_Opencv_in_Android" target="_blank" rel="external">ID-Card_with_TensorFlow_Opencv_in_Android</a></h2><h2 id="tensorflow-style-transfer-android"><a href="#tensorflow-style-transfer-android" class="headerlink" title="tensorflow-style-transfer-android"></a><a href="https://github.com/googlecodelabs/tensorflow-style-transfer-android" target="_blank" rel="external">tensorflow-style-transfer-android</a></h2><p>Artistic Style Transfer in Android using TensorFlow<br>This repository supports the codelab for Artistic Style Transfer in Android using TensorFlow. It is based on code forked from the TensorFlow repository.</p>
<h2 id="Paideia"><a href="#Paideia" class="headerlink" title="Paideia"></a><a href="https://github.com/alseambusher/Paideia" target="_blank" rel="external">Paideia</a></h2><p>Know more about your surroundings using Deep Learning <a href="http://alseambusher.github.io/Paideia" target="_blank" rel="external">http://alseambusher.github.io/Paideia</a></p>
<p>Paideia<br>Paideia aims at making lives easier for all of us by bringing knowldege that we need in day to day lives one step closer to all of us. This is an android app using which one can point their phone at quite literally anything and get information about it. For instance, when you want to know more about the fruit that is sitting beside you or when you just want to “learn” more about the stuff that is around you, Paideia helps you start off. Here is some of the info given by Paideia about random stuff that is around us:</p>
<p><img src="https://github.com/alseambusher/Paideia/raw/master/screenshots/1.png" alt=""><br><img src="https://github.com/alseambusher/Paideia/raw/master/screenshots/2.png" alt=""><br><img src="https://github.com/alseambusher/Paideia/raw/master/screenshots/3.png" alt=""><br><img src="https://github.com/alseambusher/Paideia/raw/master/screenshots/4.png" alt=""><br><img src="https://github.com/alseambusher/Paideia/raw/master/screenshots/5.png" alt=""></p>
<p>Seriously, the kind of information we get through Paideia by observing normal things around us is amazing!</p>
<p>##What can it do?</p>
<p>Detects objects around us.<br>Has inbuilt Text-to-speech system that can read it out. This can greatly help people with who are specially abled.<br>Extracts useful and relevant information from Wikipedia and Wolfram Alpha.<br>Users can set preferences which allows Paideia to customize what they see.</p>
<p>##How does it do?</p>
<p>We use a deep learning model trained using Tensorflow on Imagenet ILSVRC2012 data to recognize images from live feed.<br>We use API’s provided by Wolfram Alpha and Wikipedia in order to extract relevant information to the user.<br>We use a simple learning approach to customize feeds for users based on their usage pattern.<br>Google tts system to read out information to the user.</p>
<p>##Who does it help?</p>
<p>Children who want to learn more.<br>Adults who want to learn more.<br>Specially abled people who want to learn more.<br>So basically, everyone who wanna learn more.</p>
<p>##Where can I get it? Grab the apk from release page.</p>
<p>##Setting up the codebase.</p>
<p>Setup Tensorflow.<br>Setup Bazel.<br>Clone the repo in the root folder of tensorflow.<br>$ git clone <a href="https://github.com/alseambusher/Paideia" target="_blank" rel="external">https://github.com/alseambusher/Paideia</a><br>Get model<br>$ wget <a href="https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip" target="_blank" rel="external">https://storage.googleapis.com/download.tensorflow.org/models/inception5h.zip</a> -O /tmp/inception5h.zip</p>
<p>$ unzip /tmp/inception5h.zip -d Paideia/assets/<br>Building code<br>$ bazel build //Paideia:paideia<br>Installation<br>$ adb install bazel-bin/Paideia/paideia.apk</p>
<p>##Contribiting Pull requests and suggestions are welcome.</p>
<p><img src="https://github.com/alseambusher/Paideia/raw/master/res/drawable-xxxhdpi/ic_launcher.png" alt=""></p>
<h2 id="tensorflow-hangul-recognition"><a href="#tensorflow-hangul-recognition" class="headerlink" title="tensorflow-hangul-recognition"></a><a href="https://github.com/IBM/tensorflow-hangul-recognition" target="_blank" rel="external">tensorflow-hangul-recognition</a></h2><p>Handwritten Korean Character Recognition with TensorFlow and Android <a href="https://developer.ibm.com/code/patter…" target="_blank" rel="external">https://developer.ibm.com/code/patter…</a></p>
<p>Handwritten Korean Character Recognition with TensorFlow and Android<br>Read this in other languages: 한국어.</p>
<p>Hangul, the Korean alphabet, has 19 consonant and 21 vowel letters. Combinations of these letters give a total of 11,172 possible Hangul syllables/characters. However, only a small subset of these are typically used.</p>
<p>This journey will cover the creation process of an Android application that will utilize a TensorFlow model trained to recognize Korean syllables. In this application, users will be able to draw a Korean syllable on their mobile device, and the application will attempt to infer what the character is by using the trained model. Furthermore, users will be able to form words or sentences in the application which they can then translate using the Watson Language Translator service.</p>
<p><img src="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/hangul_tensordroid_demo1.gif" alt=""></p>
<p>The following steps will be covered:</p>
<p>Generating image data using free Hangul-supported fonts found online and elastic distortion.<br>Converting images to TFRecords format to be used for input and training of the model.<br>Training and saving the model.<br>Using the saved model in a simple Android application.<br>Connecting the Watson Language Translator service to translate the characters.<br>Flow<br><img src="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/architecture.png" alt=""></p>
<p>The user downloads several Korean fonts to use for data generation.<br>The images generated from the fonts are fed into a TensorFlow model for training.<br>The user draws a Korean character on their Android device.<br>The drawn character is recognized using the previously trained TensorFlow model and the Android TensorFlow Inference Interface.<br>A string of the classified Korean characters is sent to the Watson Language Translator service to retrieve an English translation.<br>With Watson<br>Want to take your Watson app to the next level? Looking to leverage Watson Brand assets? Join the With Watson program which provides exclusive brand, marketing, and tech resources to amplify and accelerate your Watson embedded commercial solution.</p>
<p>Included Components<br>Watson Language Translator: A Bluemix service that converts text input in one language into a destination language for the end user using background from domain-specific models.<br>TensorFlow: An open-source software library for Machine Intelligence.<br>Android: An open-source mobile operating system based on the Linux kernel.<br>Featured Technologies<br>Artificial Intelligence: Cognitive technologies that can understand, reason, learn, and interact like humans.<br>Mobile: An environment to develop apps and enable engagements that are designed specifically for mobile users.<br>Watch the <a href="https://www.youtube.com/watch?v=iefYaCOz00s" target="_blank" rel="external">Video</a><br><img src="https://camo.githubusercontent.com/cff8a356ace2a9ebd6aeb441d4bb8b68395606f8/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f6965665961434f7a3030732f302e6a7067" alt=""></p>
<p>Steps<br>Follow these steps to setup and run this developer journey. The steps are described in detail below.</p>
<p>1 Install Prerequisites<br>2 Generate Image Data<br>3 Convert Images to TFRecords<br>4 Train the Model<br>5 Try Out the Model<br>6 Create the Android Application</p>
<ol>
<li>Install Prerequisites<br>Make sure you have the python requirements for this journey installed on your system. From the root of the repository, run:</li>
</ol>
<p>pip install -r requirements.txt<br>Note: For Windows users, the scipy package is not installable via pip. The recommended way to use scipy is to install a scientific Python distribution. One of the more popular ones is Anaconda. However, you can also manually install the scipy package on Windows using one of the installers located here.</p>
<ol>
<li>Generate Image Data<br>In order to train a decent model, having copious amounts of data is necessary. However, getting a large enough dataset of actual handwritten Korean characters is challenging to find and cumbersome to create.</li>
</ol>
<p>One way to deal with this data issue is to programmatically generate the data yourself, taking advantage of the abundance of Korean font files found online. So, that is exactly what we will be doing.</p>
<p>Provided in the tools directory of this repo is hangul-image-generator.py. This script will use fonts found in the fonts directory to create several images for each character provided in the given labels file. The default labels file is 2350-common-hangul.txt which contains 2350 frequent characters derived from the KS X 1001 encoding. Other label files are 256-common-hangul.txt and 512-common-hangul.txt. These were adapted from the top 6000 Korean words compiled by the National Institute of Korean Language listed here. If you don’t have a powerful machine to train on, using a smaller label set can help reduce the amount of model training time later on.</p>
<p>The fonts folder is currently empty, so before you can generate the Hangul dataset, you must first download several font files as described in the fonts directory README. For my dataset, I used around 40 different font files, but more can always be used to improve your dataset, especially if you get several uniquely stylized ones. Once your fonts directory is populated, then you can proceed with the actual image generation with hangul-image-generator.py.</p>
<p>Optional flags for this are:</p>
<p>–label-file for specifying a different label file (perhaps with less characters). Default is ./labels/2350-common-hangul.txt.<br>–font-dir for specifying a different fonts directory. Default is ./fonts.<br>–output-dir for specifying the output directory to store generated images. Default is ./image-data.<br>Now run it, specifying your chosen label file:</p>
<p>python ./tools/hangul-image-generator.py –label-file <your label="" file="" path=""><br>Depending on how many labels and fonts there are, this script may take a while to complete. In order to bolster the dataset, three random elastic distortions are also performed on each generated character image. An example is shown below, with the original character displayed first, followed by the elastic distortions.</your></p>
<p>Normal Image Distorted Image 1 Distorted Image 2 Distorted Image 3</p>
<p>Once the script is done, the output directory will contain a hangul-images folder which will hold all the 64x64 JPEG images. The output directory will also contain a labels-map.csv file which will map all the image paths to their corresponding labels.</p>
<ol>
<li>Convert Images to TFRecords<br>The TensorFlow standard input format is TFRecords, which is a binary format that we can use to store raw image data and their labels in one place. In order to better feed in data to a TensorFlow model, let’s first create several TFRecords files from our images. A script is provided that will do this for us.</li>
</ol>
<p>This script will first read in all the image and label data based on the labels-map.csv file that was generated above. Then it will partition the data so that we have a training set and also a testing set (15% testing, 85% training). By default, the training set will be saved into multiple files/shards (three) so as not to end up with one gigantic file, but this can be configured with a CLI argument, –num-shards-train, depending on your data set size.</p>
<p>Optional flags for this script are:</p>
<p>–image-label-csv for specifying the CSV file that maps image paths to labels. Default is ./image-data/labels-map.csv<br>–label-file for specifying the labels that correspond to your training set. This is used by the script to determine the number of classes. Default is ./labels/2350-common-hangul.txt.<br>–output-dir for specifying the output directory to store TFRecords files. Default is ./tfrecords-output.<br>–num-shards-train for specifying the number of shards to divide training set TFRecords into. Default is 3.<br>–num-shards-test for specifying the number of shards to divide testing set TFRecords into. Default is 1.<br>To run the script, you can simply do:</p>
<p>python ./tools/convert-to-tfrecords.py –label-file <your label="" file="" path=""><br>Once this script has completed, you should have sharded TFRecords files in the output directory ./tfrecords-output.</your></p>
<p>$ ls ./tfrecords-output<br>test1.tfrecords    train1.tfrecords    train2.tfrecords    train3.tfrecords</p>
<ol>
<li>Train the Model<br>Now that we have a lot of data, it is time to actually use it. In the root of the project is hangul_model.py. This script will handle creating an input pipeline for reading in TFRecords files and producing random batches of images and labels. Next, a convolutional neural network (CNN) is defined, and training is performed. The training process will continuously feed in batches of images and labels to the CNN to find the optimal weight and biases for correctly classifying each character. After training, the model is exported so that it can be used in our Android application.</li>
</ol>
<p>The model here is similar to the MNIST model described on the TensorFlow website. A third convolutional layer is added to extract more features to help classify for the much greater number of classes.</p>
<p>Optional flags for this script are:</p>
<p>–label-file for specifying the labels that correspond to your training set. This is used by the script to determine the number of classes to classify for. Default is ./labels/2350-common-hangul.txt.<br>–tfrecords-dir for specifying the directory containing the TFRecords shards. Default is ./tfrecords-output.<br>–output-dir for specifying the output directory to store model checkpoints, graphs, and Protocol Buffer files. Default is ./saved-model.<br>–num-train-steps for specifying the number of training steps to perform. This should be increased with more data (or vice versa). The number of steps should cover several iterations over all of the training data (epochs). For example, if I had 320,000 images in my training set, one epoch would be 320000/100 = 3200 steps where 100 is the default batch size. So, if I wanted to train for 30 epochs, I would simply do 3200*30 = 96000 training steps. Definitely tune this parameter on your own to try and hit at least 15 epochs. Default is 30000 steps.<br>To run the training, simply do the following from the root of the project:</p>
<p>python ./hangul_model.py –label-file <your label="" file="" path=""> –num-train-steps <num><br>Depending on how many images you have, this will likely take a long time to train (several hours to maybe even a day), especially if only training on a laptop. If you have access to GPUs, these will definitely help speed things up, and you should certainly install the TensorFlow version with GPU support (supported on Ubuntu and Windows only).</num></your></p>
<p>On my Windows desktop computer with an Nvidia GTX 1080 graphics card, training about 320,000 images with the script defaults took just a bit over two hours. Training on my MacBook Pro would probably take over 20 times that long.</p>
<p>One alternative is to use a reduced label set (i.e. 256 vs 2350 Hangul characters) which can reduce the computational complexity quite a bit.</p>
<p>As the script runs, you should hopefully see the printed training accuracies grow towards 1.0, and you should also see a respectable testing accuracy after the training. When the script completes, the exported model we should use will be saved, by default, as ./saved-model/optimized_hangul_tensorflow.pb. This is a Protocol Buffer file which represents a serialized version of our model with all the learned weights and biases. This specific one is optimized for inference-only usage.</p>
<ol>
<li>Try Out the Model<br>Before we jump into making an Android application with our newly saved model, let’s first try it out. Provided is a script that will load your model and use it for inference on a given image. Try it out on images of your own, or download some of the sample images below. Just make sure each image is 64x64 pixels with a black background and white character color.</li>
</ol>
<p>Optional flags for this are:</p>
<p>–label-file for specifying a different label file. This is used to map indices in the one-hot label representations to actual characters. Default is ./labels/2350-common-hangul.txt.<br>–graph-file for specifying your saved model file. Default is ./saved-model/optimized_hangul_tensorflow.pb.<br>Run it like so:</p>
<p>python ./tools/classify-hangul.py <image path=""> –label-file <your label="" file="" path=""><br>Sample Images:</your></image></p>
<p>Sample Image 1 Sample Image 2 Sample Image 3 Sample Image 4 Sample Image 5</p>
<p>After running the script, you should see the top five predictions and their corresponding scores. Hopefully the top prediction matches what your character actually is.</p>
<p>Note: If running this script on Windows, in order for the Korean characters to be displayed on the console, you must first change the active code page to support UTF-8. Just run:</p>
<p>chcp 65001<br>Then you must change the console font to be one that supports Korean text (like Batang, Dotum, or Gulim).</p>
<ol>
<li>Create the Android Application<br>With the saved model, a simple Android application can be created that will be able to classify handwritten Hangul that a user has drawn. A completed application has already been included in ./hangul-tensordroid.</li>
</ol>
<p>Set up the project<br>The easiest way to try the app out yourself is to use Android Studio. This will take care of a lot of the Android dependencies right inside the IDE.</p>
<p>After downloading and installing Android Studio, perform the following steps:</p>
<p>Launch Android Studio<br>A Welcome to Android Studio window should appear, so here, click on Open an existing Android Studio project. If this window does not appear, then just go to File &gt; Open… in the top menu.<br>In the file browser, navigate to and click on the ./hangul-tensordroid directory of this project, and then press OK.<br>After building and initializing, the project should now be usable from within Android Studio. When Gradle builds the project for the first time, you might find that there are some dependency issues, but these are easily resolvable in Android Studio by clicking on the error prompt links to install the dependencies.</p>
<p>In Android Studio, you can easily see the project structure from the side menu.</p>
<p><img src="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/android-project-structure.png" alt=""></p>
<p>The java folder contains all the java source code for the app. Expanding this shows that we have just four java files:</p>
<p>MainActivity.java is the main launch point of the application and will handle the setup and button pressing logic.</p>
<p>PaintView.java is the class that enables the user to draw Korean characters in a BitMap on the screen.</p>
<p>HangulClassifier.java handles loading our pre-trained model and connecting it with the TensorFlow Inference Interface which we can use to pass in images for classification.</p>
<p>HangulTranslator.java interfaces with the Watson Language Translator API to get English translations for our text.</p>
<p>In it’s current state, the provided Android application uses the 2350-common-hangul.txt label files and already has a pre-trained model trained on about 320,000 images from 40 fonts. These are located in the assets folder of the project, ./hangul-tensordroid/app/src/main/assets/. If you want to switch out the model or labels file, simply place them in this directory. You must then specify the names of these files in MainActivity.java, ./hangul-tensordroid/app/src/main/java/ibm/tf/hangul/MainActivity.java, by simply changing the values of the constants LABEL_FILE and MODEL_FILE located at the top of the class.</p>
<p>If you want to enable translation support, you must do the following:</p>
<p>Create a Bluemix account here.<br>Create the Watson Language Translator service.<br>Get Translator service credentials. Credentials should have been automatically created. You can retrieve them by clicking on the Language Translator service under the Services section of your Bluemix dashboard.<br>Update ./hangul-tensordroid/app/src/main/res/values/translate_api.xml with the username and password retrieved in step 3.<br>Run the application<br>When you are ready to build and run the application, click on the green arrow button at the top of Android Studio.</p>
<p><img src="https://github.com/IBM/tensorflow-hangul-recognition/raw/master/doc/source/images/android-studio-play-button.png" alt=""></p>
<p>This should prompt a window to Select Deployment Target. If you have an actual Android device, feel free to plug it into your computer using USB. More info can be found here. If you do not have an Android device, you can alternatively use an emulator. In the Select Deployment Target window, click on Create New Virtual Device. Then just follow the wizard, selecting a device definition and image (preferably an image with API level 21 or above). After the virtual device has been created, you can now select it when running the application.</p>
<p>After selecting a device, the application will automatically build, install, and then launch on the device.</p>
<p>Try drawing in the application to see how well the model recognizes your Hangul writing.</p>
<p>Links<br>Deep MNIST for Experts: Tutorial for creating and training a convolutional neural network to recognize handwritten digits.<br>TensorFlow Mobile: Information on TensorFlow mobile support on different platforms.<br>Hangul Syllables: List of all Hangul syllables.</p>
<h2 id="ai-candy-dispenser"><a href="#ai-candy-dispenser" class="headerlink" title="ai-candy-dispenser"></a><a href="https://github.com/alvarowolfx/ai-candy-dispenser" target="_blank" rel="external">ai-candy-dispenser</a></h2><p>A Candy Dispenser using Android Things + TensorFlow</p>
<p>Android Things A.I. Candy Dispenser<br>The Android Things A.I Candy Dispenser it’s a demonstration of how to create a “smart” candy machine. The device have a game that ask the user for a specific thing like a Bird, Dog ou Cat and the user should show a photo of that thing in the predefined time to win candies.</p>
<p>This project uses a button to interact with the user, obtains images via a camera peripheral and a modified electric candy dispenser being controlled by a GPIO with a transistor.</p>
<p>When the user takes a picture, it processes the image data using Google’s Cloud Vision API, which returns annotations and metadata of the image. This info is used by the device to see if it matches what it requested. When we have a match, the motor of the candy machine is activated to give the user the prize.</p>
<p>All users interface is presented in a Serial i2C 20x4 Display.</p>
<p>Pre-requisites<br>Android Things compatible board<br>Android Things compatible camera (for example, the Raspberry Pi 3 camera module)<br>Android Studio 2.2+<br>“Google Repository” from the Android SDK Manager<br>Google Cloud project with Cloud Vision API enabled<br>The following individual components:<br>1 push button<br>1 resistor<br>1 electric candy machine<br>1 NPN transistor<br>1 diode<br>1 Serial i2c 20x4 Display<br>jumper wires<br>1 breadboard<br>Schematics<br><img src="https://github.com/alvarowolfx/ai-candy-dispenser/raw/master/Schematic.png" alt=""></p>
<p>Setup and Build<br>To setup, follow these steps below.</p>
<p>Add a valid Google Cloud Vision API key in the constant ImageClassifierUtil.CLOUD_VISION_API_KEY<br>Create a Google Cloud Platform (GCP) project on GCP Console<br>Enable Cloud Vision API under Library<br>Add an API key under Credentials<br>Copy and paste the Cloud Vision API key to the constant in ImageClassifierUtil.kt<br>Running<br>To run the app module on an Android Things board:</p>
<p>Connect a push button to your device’s GPIO pin according to the schematics below<br>Deploy and run the app module<br>Reboot the Android Things board in order to grant the camera permission (this is a known issue with Developer Preview )<br>Press the button to start the game and see what it asks.<br>Search for a photo and press the button to take a picture for it.<br>Wait for the results, if it succeeds it will ask you to press the button to release the candies.<br>References<br><a href="https://github.com/androidthings/doorbell" target="_blank" rel="external">https://github.com/androidthings/doorbell</a><br><a href="https://github.com/androidthings/sample-tensorflow-imageclassifier" target="_blank" rel="external">https://github.com/androidthings/sample-tensorflow-imageclassifier</a></p>
<h2 id="TensorflowLite"><a href="#TensorflowLite" class="headerlink" title="TensorflowLite"></a><a href="https://github.com/lizhangqu/TensorflowLite" target="_blank" rel="external">TensorflowLite</a></h2><p>Tensorflow Lite Android Library</p>
<h2 id="RapidDraw"><a href="#RapidDraw" class="headerlink" title="RapidDraw"></a><a href="https://github.com/C-Aniruddh/RapidDraw" target="_blank" rel="external">RapidDraw</a></h2><p>A simple artificial intelligence experiment to find out if mobile neural networks can recognize human-made doodles</p>
<p>RapidDraw<br>RapidDraw is a simple Artificial Intelligence experiment to check if a neural network running on Android can recognize what you are doodling. To make it more fun, there is an entire game wrapped around the neural network.</p>
<p>On each attempt, you’ll be given 1 chance to draw 8 different objects. The objects will be chosen randomly from over 100 objects.</p>
<p>Get it here : <a href="https://play.google.com/store/apps/details?id=io.aniruddh.rapiddraw" target="_blank" rel="external">https://play.google.com/store/apps/details?id=io.aniruddh.rapiddraw</a></p>
<p>Building<br>Building this is pretty easy. Simply import the files in Android Studio and you are good to go.</p>
<p>Though there are somethings you have to do yourself :</p>
<p>Train the dataset : The retrained_graph.pb and retrained_labels.txt have not been provided in this source. So you’ll have to train the neural network yourself to get these files out. Refer to <a href="https://github.com/C-Aniruddh/RapidDraw/tree/in-dev/processing" target="_blank" rel="external">processing</a> for details.<br>Contributing<br>Fork it<br>Make necessary changes<br>Make a pull request<br>I’ll merge it if it serves the purpose ;)</p>
<h2 id="driverless-rccar"><a href="#driverless-rccar" class="headerlink" title="driverless-rccar"></a><a href="https://github.com/gokhanettin/driverless-rccar" target="_blank" rel="external">driverless-rccar</a></h2><p>DRIVERLESS RC CAR<br><img src="https://github.com/gokhanettin/driverless-rccar/raw/master/img/rccar.jpg" alt=""></p>
<p>Because real cars are too expensive for self-driving car studies, we are building a self-driving RC car as a testbed. When you have</p>
<p>an RC car with a steering servo,<br>an Android device with WiFi, Camera and Bluetooth support (we use Samsung Galaxy S3 Mini),<br>a selfie stick,<br>a breadboard and many jumpers,<br>an Arduino (we have an Arduino UNO),<br>an HC-06 bluetooth module,<br>a powerful GPU at your disposal (we use GTX 1070)<br>you are good to go. We also have a speed sensor, but it is only for monitoring purposes, you don’t need it to train and drive your RC car.</p>
<p>Data Acquisition<br>While a human controlling the car through a transmitter, Arduino reads steering and speed commands, which are essentially servo signals, from the receiver. It also computes steering angles (just a linear mapping from steering commands to angles in degrees) and measures speed in m/s from the speed sensor. Arduino eventually sends them to the Android device installed on the car. We use bluetooth for the communication between Arduino and Android. Android device attaches a current image frame to the Arduino readings and forwards them all to a remote machine over TCP/IP. We capture 20 frames per second along with the Arduino readings. We had to create an access point on the remote machine to make the communication more reliable.</p>
<p>The figure below demostrates how the data acquisition works.<br><img src="https://github.com/gokhanettin/driverless-rccar/raw/master/img/data-acquisition.jpg" alt=""></p>
<p>This is how our dataset looks like. The commands marked in red are the steering commands. We have speed commands (i.e. throttle) and image file names on the right and left of the red rectangle, respectively.</p>
<p><img src="https://github.com/gokhanettin/driverless-rccar/raw/master/img/dataset.jpg" alt=""></p>
<p>A row in dataset.txt is as follows:</p>
<p>timestep;timestamp;imagefile;steering_cmd;speed_cmd;steering;speed<br>Teaching a Machine to Steer a Car<br>You might have heard about Udacity’s open source self-driving car project. They organize various challenges to build the car step by step. Their steering control challenge led to brilliant deep learning models and we implemented the winner model by Team Komanda for our RC car. It has been a great starting point for our self-driving RC car journey. It should also be possible to experiment with other existing steering control models or develop and try out our own models using our RC car setup.</p>
<p>One difference between our training strategy and original Komanda model is that we augment our training set by vertically flipping images and mirroring their corresponding steering commands and angles with respect to the neutral position before training. We don’t use steering angles for training, they are used for monitoring purposes just like speed values in m/s. Another difference is the size of input images. Our images have a size of 176x144, which was the minimum possible image size from our Android device. The model tries to predict both steering and speed commands given the input images.</p>
<p>You can see the results in the following video.</p>
<p><img src="https://camo.githubusercontent.com/d7c922af7b5102f1af949409b9b22ff951a8fcba/68747470733a2f2f696d672e796f75747562652e636f6d2f76692f5a36564b646f46325a58592f302e6a7067" alt=""></p>
<p>Directory Structure<br>carino contains Arduino-specific code.<br>caroid contains Android-specific code.<br>station contains dataset acquisition code and our deep learning agent(s) on remote machine side. It also features various visualization tools.</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Android/" rel="tag"># Android</a>
          
            <a href="/tags/AI/" rel="tag"># AI</a>
          
            <a href="/tags/Tensorflow/" rel="tag"># Tensorflow</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/02/18/Book/The Elegant Pitch/" rel="next" title="The Elegant Pitch">
                <i class="fa fa-chevron-left"></i> The Elegant Pitch
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/02/19/Concept/S/Subconsciousness/" rel="prev" title="Subconsciousness">
                Subconsciousness <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
        
<script>
  with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='//bdimg.share.baidu.com/static/api/js/share.js?cdnversion='+~(-new Date()/36e5)];
</script>

      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      <div id="hypercomments_widget"></div>
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/img/headonhill.jpg"
                alt="IPCreator" />
            
              <p class="site-author-name" itemprop="name">IPCreator</p>
              <p class="site-description motion-element" itemprop="description">For life, Keep Growing Up is the only Password.</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">1018</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">26</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">1160</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/aiseeme" target="_blank" title="github">
                    
                      <i class="fa fa-fw fa-globe"></i>github</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://weibo.com/zhuxuanlvzxl/home?wvr=5&lf=reg" target="_blank" title="weibo">
                    
                      <i class="fa fa-fw fa-globe"></i>weibo</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="http://blog.163.com/zhuxuanlv@126/" target="_blank" title="blog">
                    
                      <i class="fa fa-fw fa-globe"></i>blog</a>
                </span>
              
            
          </div>

          
          
            <div class="cc-license motion-element" itemprop="license">
              <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" target="_blank">
                <img src="/images/cc-by-nc-sa.svg" alt="Creative Commons" />
              </a>
            </div>
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://book.douban.com/subject/27078435/" title="transition" target="_blank">transition</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://h5.sao.cn/product/detail?alias=onhnfsdtybxb" title="leading to freedom of wealth" target="_blank">leading to freedom of wealth</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://zhibimo.com/read/xiaolai/ba-shi-jian-dang-zuo-peng-you/index.html" title="take time as a friend" target="_blank">take time as a friend</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://zhibimo.com/read/xiaolai/reborn-every-7-years/" title="reborn every 7 years" target="_blank">reborn every 7 years</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#开源网站：Github"><span class="nav-number">1.</span> <span class="nav-text">开源网站：Github</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#TSFOnAndroid"><span class="nav-number">1.1.</span> <span class="nav-text">TSFOnAndroid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow集成Android工程的框架"><span class="nav-number">1.2.</span> <span class="nav-text">TensorFlow集成Android工程的框架</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#AndroidTensorFlowMachineLearningExample"><span class="nav-number">1.3.</span> <span class="nav-text">AndroidTensorFlowMachineLearningExample</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Android-TensorFlow-MachineLearning-MNIST-Example-Building-Model-with-TensorFlow-for-Android"><span class="nav-number">1.4.</span> <span class="nav-text">Android TensorFlow MachineLearning MNIST Example (Building Model with TensorFlow for Android)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HyperLPR"><span class="nav-number">1.5.</span> <span class="nav-text">HyperLPR</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hyperlpr4Android"><span class="nav-number">1.6.</span> <span class="nav-text">hyperlpr4Android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mnist-android-tensorflow"><span class="nav-number">1.7.</span> <span class="nav-text">mnist-android-tensorflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#android-yolo"><span class="nav-number">1.8.</span> <span class="nav-text">android-yolo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#yolo-tiny-v1-mobile"><span class="nav-number">1.9.</span> <span class="nav-text">yolo-tiny-v1-mobile</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlowAndroidDemo"><span class="nav-number">1.10.</span> <span class="nav-text">TensorFlowAndroidDemo</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Tensorflow-MNIST-demo-on-Android"><span class="nav-number">1.11.</span> <span class="nav-text">Tensorflow MNIST demo on Android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Human-Activity-Recognition-using-CNN"><span class="nav-number">1.12.</span> <span class="nav-text">Human-Activity-Recognition-using-CNN</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Guide-to-Running-Tensorflow-Models-on-Android"><span class="nav-number">1.13.</span> <span class="nav-text">A_Guide_to_Running_Tensorflow_Models_on_Android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs"><span class="nav-number">1.14.</span> <span class="nav-text">TensorFlow-on-Android-for-Human-Activity-Recognition-with-LSTMs</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras"><span class="nav-number">1.15.</span> <span class="nav-text">Credit-Card-Fraud-Detection-using-Autoencoders-in-Keras</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Real-Time-Semantic-Segmentation-in-Mobile-device"><span class="nav-number">1.16.</span> <span class="nav-text">Real-Time Semantic Segmentation in Mobile device</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#NewFeelings"><span class="nav-number">1.17.</span> <span class="nav-text">NewFeelings</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow-classifier-android"><span class="nav-number">1.18.</span> <span class="nav-text">tensorflow-classifier-android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ck-tensorflow"><span class="nav-number">1.19.</span> <span class="nav-text">ck-tensorflow</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TFDroid"><span class="nav-number">1.20.</span> <span class="nav-text">TFDroid</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ID-Card-with-TensorFlow-Opencv-in-Android"><span class="nav-number">1.21.</span> <span class="nav-text">ID-Card_with_TensorFlow_Opencv_in_Android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow-style-transfer-android"><span class="nav-number">1.22.</span> <span class="nav-text">tensorflow-style-transfer-android</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Paideia"><span class="nav-number">1.23.</span> <span class="nav-text">Paideia</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#tensorflow-hangul-recognition"><span class="nav-number">1.24.</span> <span class="nav-text">tensorflow-hangul-recognition</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ai-candy-dispenser"><span class="nav-number">1.25.</span> <span class="nav-text">ai-candy-dispenser</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TensorflowLite"><span class="nav-number">1.26.</span> <span class="nav-text">TensorflowLite</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RapidDraw"><span class="nav-number">1.27.</span> <span class="nav-text">RapidDraw</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#driverless-rccar"><span class="nav-number">1.28.</span> <span class="nav-text">driverless-rccar</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">IPCreator</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.3</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	

		<script type="text/javascript">
		_hcwp = window._hcwp || [];

		_hcwp.push({widget:"Bloggerstream", widget_id: 96030, selector:".hc-comment-count", label: "{\%COUNT%\}" });

		
		_hcwp.push({widget:"Stream", widget_id: 96030, xid: "2018/02/18/Technology/AI/Tensorflow and Android/"});
		

		(function() {
		if("HC_LOAD_INIT" in window)return;
		HC_LOAD_INIT = true;
		var lang = (navigator.language || navigator.systemLanguage || navigator.userLanguage || "en").substr(0, 2).toLowerCase();
		var hcc = document.createElement("script"); hcc.type = "text/javascript"; hcc.async = true;
		hcc.src = ("https:" == document.location.protocol ? "https" : "http")+"://w.hypercomments.com/widget/hc/96030/"+lang+"/widget.js";
		var s = document.getElementsByTagName("script")[0];
		s.parentNode.insertBefore(hcc, s.nextSibling);
		})();
		</script>

	
















  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("TfzAWJSzk2L4jpj4gHaNsMbp-gzGzoHsz", "PXNkGhS05PxL7rV3N8gDlAQe");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
